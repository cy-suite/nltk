.. Copyright (C) 2001-2015 NLTK Project
.. For license information, see LICENSE.TXT

    >>> from __future__ import print_function
    >>> from nltk.tokenize import *

Regression Tests: Treebank Tokenizer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Some test strings.

    >>> s1 = "On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88."
    >>> word_tokenize(s1)
    ['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366.88', '.']
    >>> s2 = "\"We beat some pretty good teams to get here,\" Slocum said."
    >>> word_tokenize(s2)
    ['``', 'We', 'beat', 'some', 'pretty', 'good', 'teams', 'to', 'get', 'here', ',', "''", 'Slocum', 'said', '.']
    >>> s3 = "Well, we couldn't have this predictable, cliche-ridden, \"Touched by an Angel\" (a show creator John Masius worked on) wanna-be if she didn't."
    >>> word_tokenize(s3)
    ['Well', ',', 'we', 'could', "n't", 'have', 'this', 'predictable', ',', 'cliche-ridden', ',', '``', 'Touched', 'by', 'an', 'Angel', "''", '(', 'a', 'show', 'creator', 'John', 'Masius', 'worked', 'on', ')', 'wanna-be', 'if', 'she', 'did', "n't", '.']
    >>> s4 = "I cannot cannot work under these conditions!"
    >>> word_tokenize(s4)
    ['I', 'can', 'not', 'can', 'not', 'work', 'under', 'these', 'conditions', '!']
    >>> s5 = "The company spent $30,000,000 last year."
    >>> word_tokenize(s5)
    ['The', 'company', 'spent', '$', '30,000,000', 'last', 'year', '.']
    >>> s6 = "The company spent 40.75% of its income last year."
    >>> word_tokenize(s6)
    ['The', 'company', 'spent', '40.75', '%', 'of', 'its', 'income', 'last', 'year', '.']
    >>> s7 = "He arrived at 3:00 pm."
    >>> word_tokenize(s7)
    ['He', 'arrived', 'at', '3:00', 'pm', '.']
    >>> s8 = "I bought these items: books, pencils, and pens."
    >>> word_tokenize(s8)
    ['I', 'bought', 'these', 'items', ':', 'books', ',', 'pencils', ',', 'and', 'pens', '.']
    >>> s9 = "Though there were 150, 100 of them were old."
    >>> word_tokenize(s9)
    ['Though', 'there', 'were', '150', ',', '100', 'of', 'them', 'were', 'old', '.']
    >>> s10 = "There were 300,000, but that wasn't enough."
    >>> word_tokenize(s10)
    ['There', 'were', '300,000', ',', 'but', 'that', 'was', "n't", 'enough', '.']

Sentence tokenization in word_tokenize:

    >>> s11 = "I called Dr. Jones. I called Dr. Jones."
    >>> word_tokenize(s11)
    ['I', 'called', 'Dr.', 'Jones', '.', 'I', 'called', 'Dr.', 'Jones', '.']
    >>> s12 = ("Ich muss unbedingt daran denken, Mehl, usw. fur einen "
    ...        "Kuchen einzukaufen. Ich muss.")
    >>> word_tokenize(s12)
    ['Ich', 'muss', 'unbedingt', 'daran', 'denken', ',', 'Mehl', ',', 'usw',
     '.', 'fur', 'einen', 'Kuchen', 'einzukaufen', '.', 'Ich', 'muss', '.']
    >>> word_tokenize(s12, 'german')
    ['Ich', 'muss', 'unbedingt', 'daran', 'denken', ',', 'Mehl', ',', 'usw.',
     'fur', 'einen', 'Kuchen', 'einzukaufen', '.', 'Ich', 'muss', '.']


Regression Tests: Regexp Tokenizer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Some additional test strings.

    >>> s = ("Good muffins cost $3.88\nin New York.  Please buy me\n"
    ...      "two of them.\n\nThanks.")
    >>> s2 = ("Alas, it has not rained today. When, do you think, "
    ...       "will it rain again?")
    >>> s3 = ("<p>Although this is <b>not</b> the case here, we must "
    ...       "not relax our vigilance!</p>")

    >>> regexp_tokenize(s2, r'[,\.\?!"]\s*', gaps=False)
    [', ', '. ', ', ', ', ', '?']
    >>> regexp_tokenize(s2, r'[,\.\?!"]\s*', gaps=True)
    ['Alas', 'it has not rained today', 'When', 'do you think',
     'will it rain again']

Take care to avoid using capturing groups:

    >>> regexp_tokenize(s3, r'</?[bp]>', gaps=False)
    ['<p>', '<b>', '</b>', '</p>']
    >>> regexp_tokenize(s3, r'</?(?:b|p)>', gaps=False)
    ['<p>', '<b>', '</b>', '</p>']
    >>> regexp_tokenize(s3, r'</?(?:b|p)>', gaps=True)
    ['Although this is ', 'not',
     ' the case here, we must not relax our vigilance!']

Named groups are capturing groups, and confuse the tokenizer:

    >>> regexp_tokenize(s3, r'</?(?P<named>b|p)>', gaps=False)
    ['p', 'b', 'b', 'p']
    >>> regexp_tokenize(s3, r'</?(?P<named>b|p)>', gaps=True)
    ['p', 'Although this is ', 'b', 'not', 'b',
     ' the case here, we must not relax our vigilance!', 'p']

Make sure that nested groups don't confuse the tokenizer:

    >>> regexp_tokenize(s2, r'(?:h|r|l)a(?:s|(?:i|n0))', gaps=False)
    ['las', 'has', 'rai', 'rai']
    >>> regexp_tokenize(s2, r'(?:h|r|l)a(?:s|(?:i|n0))', gaps=True)
    ['A', ', it ', ' not ', 'ned today. When, do you think, will it ',
     'n again?']

Back-references require capturing groups, and these are not supported:

    >>> regexp_tokenize("aabbbcccc", r'(.)\1')
    ['a', 'b', 'c', 'c']

A simple sentence tokenizer '\.(\s+|$)'

    >>> regexp_tokenize(s, pattern=r'\.(?:\s+|$)', gaps=True)
    ['Good muffins cost $3.88\nin New York',
     'Please buy me\ntwo of them', 'Thanks']

Post-hoc alignment of tokens with a source string

    >>> from nltk.tokenize.util import *
    >>> list(align_tokens(word_tokenize(s1), s1))
    [(0, 2), (3, 4), (5, 6), (6, 12), (13, 21), (22, 24), (25, 27), (28, 33), (34, 36), (37, 38), (39, 46), (46, 47), (48, 51), (52, 59), (60, 67), (68, 73), (74, 76), (77, 78), (78, 84), (84, 85)]
    >>> list(align_tokens([''], ""))
    [(0, 0)]
    >>> list(align_tokens([''], " "))
    [(0, 0)]
    >>> list(align_tokens([], ""))
    []
    >>> list(align_tokens([], " "))
    []
    >>> list(align_tokens(['a'], "a"))
    [(0, 1)]
    >>> list(align_tokens(['abc', 'def'], "abcdef"))
    [(0, 3), (3, 6)]
    >>> list(align_tokens(['abc', 'def'], "abc def"))
    [(0, 3), (4, 7)]
    >>> list(align_tokens(['ab', 'cd'], "ab cd ef"))
    [(0, 2), (3, 5)]
    >>> list(align_tokens(['ab', 'cd', 'ef'], "ab cd ef"))
    [(0, 2), (3, 5), (6, 8)]
    >>> list(align_tokens(['ab', 'cd', 'efg'], "ab cd ef"))
    Traceback (most recent call last):
    ....
    ValueError: ('Sentence ended while scanning for token start', 2)
    >>> list(align_tokens(['ab', 'cd', 'ef', 'gh'], "ab cd ef"))
    Traceback (most recent call last):
    ....
    ValueError: ('Sentence ended while scanning for token start', 3)
    >>> list(align_tokens(['The', 'plane', ',', 'bound', 'for', 'St', 'Petersburg', ',', 'crashed', 'in', 'Egypt', "'s", 'Sinai', 'desert', 'just', '23', 'minutes', 'after', 'take-off', 'from', 'Sharm', 'el-Sheikh', 'on', 'Saturday', '.'], "The plane, bound for St Petersburg, crashed in Egypt's Sinai desert just 23 minutes after take-off from Sharm el-Sheikh on Saturday."))
    [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23), (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54), (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89), (90, 98), (99, 103), (104, 109), (110, 119), (120, 122), (123, 131), (131, 132)]
